{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1027cb2b",
      "metadata": {
        "id": "1027cb2b"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title Install Prerequisites Packages\n",
        "\n",
        "# Install yt-dlp to handle YouTube URLs\n",
        "!pip install -q yt-dlp\n",
        "# Install the FFmpeg Python wrapper\n",
        "!pip install ffmpeg-python\n",
        "# Install the command-line tool (essential for the Python wrapper)\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# Install whisperx\n",
        "!pip3 install git+https://github.com/m-bain/whisperx.git -q\n",
        "# Install Hugging Face Package\n",
        "!pip3 install -u huggingface_hub -q\n",
        "# Install libcudnn8\n",
        "!apt-get install libcudnn8 -q\n",
        "\n",
        "# Install ollama and PCI Utilities\n",
        "!sudo apt-get install pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "# Install Ollama Python library\n",
        "!pip install -q ollama\n",
        "\n",
        "# Recommended installs for Knowledge Base\n",
        "!pip install sentence-transformers chromadb langchain-text-splitters\n",
        "!pip install duckduckgo-search beautifulsoup4 readability-lxml tldextract httpx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title Load Libraries and Initialize Directories, ChromaDB & Embedder\n",
        "import json, os, re, time, tldextract, hashlib, traceback, yt_dlp, requests, subprocess\n",
        "from bs4 import BeautifulSoup\n",
        "from readability import Document\n",
        "from pathlib import Path\n",
        "import httpx, chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from duckduckgo_search import DDGS\n",
        "from typing import List, Dict, Any, Tuple, List, Union\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Initialize Directories\n",
        "KB_DIR, AUDIO_DIR, WHISPERX_DIR, LLM_DIR = \"kb_data\", \"audio_data\", \"whisperx_data\", \"llm_data\"\n",
        "for d in [KB_DIR, AUDIO_DIR, WHISPERX_DIR, LLM_DIR]: Path(d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Initialize ChromaDB and Embedder\n",
        "# loads a sentence-transformers model for multilingual embeddings\n",
        "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# creates a persistent Chroma database at KB_DIR. allow_reset=True can reset/overwrite DB on some setups\n",
        "chroma_client = chromadb.PersistentClient(path=KB_DIR, settings=Settings(allow_reset=True))\n",
        "\n",
        "# gets or creates a collection named \"yt_kb\" (vector collection) using cosine similarity\n",
        "kb = chroma_client.get_or_create_collection(name=\"yt_kb\", metadata={\"hnsw:space\": \"cosine\"})\n"
      ],
      "metadata": {
        "id": "MvNpRjMPcFok"
      },
      "id": "MvNpRjMPcFok",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Audio Extraction\n",
        "import subprocess, os, ffmpeg\n",
        "\n",
        "def extract_youtube_audio(youtube_url, output_dir):\n",
        "  \"\"\" Downloads and processes audio from YouTube. \"\"\"\n",
        "  ffmpeg_executable_path = '/usr/bin/ffmpeg'\n",
        "\n",
        "  try:\n",
        "      os.makedirs(output_dir, exist_ok=True); print(f\"Output directory '{output_dir}' created.\")\n",
        "      result = subprocess.run([\"yt-dlp\", \"-f\", \"bestaudio[ext=m4a]/bestaudio\", \"--get-url\", youtube_url], capture_output=True, text=True, check=True)\n",
        "      audio_url = result.stdout.strip()\n",
        "\n",
        "      if not audio_url: print(\"Failed to get direct audio stream URL.\"); return None\n",
        "      print(f\"Successfully extracted audio URL from {youtube_url}\")\n",
        "\n",
        "      audio_output = os.path.join(output_dir, 'audio.wav')\n",
        "      (ffmpeg.input(audio_url).output(audio_output, vn=None).global_args('-loglevel','error').run(overwrite_output=True, cmd=ffmpeg_executable_path))\n",
        "      print(\"Successfully converted audio to WAV format\"); return audio_output\n",
        "\n",
        "  except subprocess.CalledProcessError as e: print(f\"yt-dlp failed to extract URL: {e}\"); return None"
      ],
      "metadata": {
        "id": "DWsXmjl6TDb6"
      },
      "id": "DWsXmjl6TDb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Audio Transcription\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "import subprocess\n",
        "\n",
        "def transcribe_audio(audio_file: str, output_dir: str, hf_token: str, chunk_sizes: list[int], output_format: str = \"srt\"):\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    transcript_paths = {}\n",
        "\n",
        "    for chunk_size in chunk_sizes:\n",
        "        print(f\"\\n▶ Processing chunk size {chunk_size} sec\")\n",
        "        srt_file = output_dir / f\"transcript_chunk{chunk_size}.srt\"\n",
        "\n",
        "        try:\n",
        "            subprocess.run([\n",
        "                \"whisperx\", \"--model\", \"large-v2\", \"--device\", \"cuda\",\n",
        "                \"--compute_type\", \"float16\", \"--chunk_size\", str(chunk_size),\n",
        "                \"--diarize\", \"--hf_token\", hf_token, \"--language\", \"ta\",\n",
        "                \"--align_model\", \"Amrrs/wav2vec2-large-xlsr-53-tamil\",\n",
        "                \"--output_dir\", str(output_dir), \"--output_format\", \"srt\", audio_file\n",
        "            ], check=True)\n",
        "\n",
        "            if (output_file := output_dir / \"audio.srt\").exists():\n",
        "                output_file.rename(srt_file)\n",
        "                print(f\"Saved: {srt_file}\")\n",
        "                transcript_paths[chunk_size] = srt_file\n",
        "            else:\n",
        "                print(\"WhisperX output file not found.\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error for chunk size {chunk_size}: {e}\")\n",
        "\n",
        "    return transcript_paths"
      ],
      "metadata": {
        "id": "tESxQLj2kBCP"
      },
      "id": "tESxQLj2kBCP",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Start Ollama using nohup\n",
        "!nohup ollama serve > ollama.log 2>&1 &"
      ],
      "metadata": {
        "id": "kseowNW_uPyR"
      },
      "id": "kseowNW_uPyR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pull LLM and Initiate Ollama Client\n",
        "import ollama\n",
        "ollama.pull('gemma3:12b') # Pull the model\n",
        "llm_model = 'gemma3:12b' # Set model name for later use\n"
      ],
      "metadata": {
        "id": "I01x8H62uTdX"
      },
      "id": "I01x8H62uTdX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title Helper functions\n",
        "from typing import List, Dict, Any\n",
        "import hashlib\n",
        "\n",
        "_id_for = lambda text: hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
        "    return embedder.encode(texts, normalize_embeddings=True).tolist()\n",
        "\n",
        "def chunk_text(text: str, max_chars: int = 500) -> List[str]:\n",
        "    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n",
        "\n",
        "def upsert_to_kb(texts: List[str], metadatas: List[Dict[str, Any]], chunk_size: int = 500):\n",
        "    if not texts: return\n",
        "\n",
        "    all_chunks, all_metas = [], []\n",
        "    for text, meta in zip(texts, metadatas):\n",
        "        chunks = chunk_text(text, chunk_size)\n",
        "        all_chunks.extend(chunks)\n",
        "        all_metas.extend([meta.copy() for _ in chunks])\n",
        "\n",
        "    embeddings = embed_texts(all_chunks)\n",
        "\n",
        "    if len(all_chunks) != len(embeddings) != len(all_metas):\n",
        "        raise ValueError(f\"Length mismatch: chunks={len(all_chunks)}, embeddings={len(embeddings)}, metas={len(all_metas)}\")\n",
        "\n",
        "    ids = [_id_for(f\"{m.get('source_url','')}|{m.get('entity','')}|{chunk[:50]}|{i}\")\n",
        "           for i, (chunk, m) in enumerate(zip(all_chunks, all_metas))]\n",
        "\n",
        "    kb.upsert(ids=ids, documents=all_chunks, metadatas=all_metas, embeddings=embeddings)"
      ],
      "metadata": {
        "id": "ka0xRHXevJLs"
      },
      "id": "ka0xRHXevJLs",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title 3. YouTube Page Scraping and NER Extraction\n",
        "\n",
        "def default_yt_data(url):\n",
        "    return {\"url\": url, \"type\": \"youtube_metadata\", \"title\": \"Unknown Title\",\n",
        "            \"uploader\": \"Unknown Channel\", \"upload_date\": \"\", \"description\": \"\",\n",
        "            \"tags\": [], \"view_count\": 0, \"like_count\": 0, \"categories\": [], \"webpage_url\": url}\n",
        "\n",
        "def empty_ner():\n",
        "    return {k: [] for k in [\"people\",\"organizations\",\"movies_or_shows\",\"awards\",\"places\",\"others\"]}\n",
        "\n",
        "def scrape_youtube_extract_ner(youtube_url: str):\n",
        "    \"\"\"Scrapes YouTube video, extracts NER, and performs web searches for extracted entities.\"\"\"\n",
        "    yt_data = {\"url\": youtube_url, \"type\": \"youtube_metadata\"}\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL({\"quiet\": True, \"skip_download\": True}) as ydl:\n",
        "            info = ydl.extract_info(youtube_url, download=False)\n",
        "            yt_data.update({k: info.get(k,\"\") for k in [\"title\",\"uploader\",\"upload_date\",\"description\",\"webpage_url\"]})\n",
        "            yt_data.update({k: info.get(k,[]) for k in [\"tags\",\"categories\"]})\n",
        "            yt_data.update({k: info.get(k,0) for k in [\"view_count\",\"like_count\"]})\n",
        "    except Exception as e:\n",
        "        print(f\"YouTube metadata extraction failed: {e}\"); yt_data = default_yt_data(youtube_url)\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(yt_data[\"webpage_url\"], timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        soup = BeautifulSoup(Document(resp.text).summary() or resp.text, \"html.parser\")\n",
        "        page_text = soup.get_text(\" \", strip=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Webpage scraping failed: {e}\"); page_text = \"\"\n",
        "\n",
        "    if yt_data.get(\"title\") or yt_data.get(\"description\"):\n",
        "        yt_text = (f\"Title: {yt_data.get('title','N/A')}\\nChannel: {yt_data.get('uploader','N/A')}\\n\"\n",
        "                   f\"Upload Date: {yt_data.get('upload_date','N/A')}\\nViews: {yt_data.get('view_count','N/A')}\\n\"\n",
        "                   f\"Likes: {yt_data.get('like_count','N/A')}\\nDescription: {yt_data.get('description','')[:1000]}\\n\"\n",
        "                   f\"Tags: {', '.join(yt_data.get('tags',[])) or 'N/A'}\\nCategories: {', '.join(yt_data.get('categories',[])) or 'N/A'}\")\n",
        "        metadata = {k:v for k,v in {\n",
        "            \"source_url\": yt_data.get(\"webpage_url\", youtube_url),\n",
        "            \"entity\": \"__youtube_metadata__\",\"type\":\"youtube_metadata\",\n",
        "            \"title\": yt_data.get(\"title\",\"Unknown Title\"), \"uploader\": yt_data.get(\"uploader\"),\n",
        "            \"upload_date\": yt_data.get(\"upload_date\"), \"view_count\": yt_data.get(\"view_count\"),\n",
        "            \"like_count\": yt_data.get(\"like_count\"), \"tags\": ', '.join(yt_data.get(\"tags\",[])),\n",
        "            \"categories\": ', '.join(yt_data.get(\"categories\",[]))\n",
        "        }.items() if v is not None}\n",
        "        upsert_to_kb([yt_text],[metadata])\n",
        "\n",
        "    ner_prompt = f\"\"\"Extract UNIQUE named entities from this Tamil/English content as JSON with: people, organizations, movies_or_shows, awards, places, others.\n",
        "\n",
        "CONTENT SOURCES:\n",
        "TITLE: {yt_data.get('title','')}\n",
        "DESCRIPTION: {yt_data.get('description','')[:3000]}\n",
        "TAGS: {', '.join(yt_data.get('tags', []))}\n",
        "CATEGORIES: {', '.join(yt_data.get('categories', []))}\n",
        "PAGE_TEXT: {page_text[:8000]}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Extract named entities from ALL content sources above\n",
        "- Tags and categories may contain important entities\n",
        "- Remove duplicates across different sources\n",
        "- Return ONLY valid JSON without any additional text\n",
        "- JSON format: {{\"people\": [], \"organizations\": [], \"movies_or_shows\": [], \"awards\": [], \"places\": [], \"others\": []}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = ollama.generate(model=llm_model, prompt=ner_prompt, options={'temperature':0.1,'num_predict':1000})\n",
        "        ner_response = resp['response']; print(f\"Raw LLM Response: {ner_response[:200]}...\")\n",
        "        ner = json.loads(re.search(r'\\{[\\s\\S]*\\}', ner_response).group(0)); print(\"NER extraction successful!\")\n",
        "    except Exception as e:\n",
        "        print(f\"NER extraction failed: {e}\"); ner = empty_ner()\n",
        "\n",
        "    entity_docs = []\n",
        "    with DDGS() as ddgs:\n",
        "        for cat, ents in ner.items():\n",
        "            for ent in ents:\n",
        "                try:\n",
        "                    for res in list(ddgs.text(ent, max_results=2)):\n",
        "                        if url := res.get(\"href\"):\n",
        "                            try:\n",
        "                                r = requests.get(url, timeout=10, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
        "                                text = BeautifulSoup(r.text,'html.parser').get_text()\n",
        "                                entity_docs.append((\n",
        "                                    f\"Entity: {ent}\\nCategory: {cat}\\nSource: {url}\\nContent: {text[:2000]}\",\n",
        "                                    {\"source_url\":url,\"entity\":ent,\"entity_category\":cat,\"type\":\"web_context\",\"title\":res.get(\"title\",\"\")}\n",
        "                                ))\n",
        "                            except requests.exceptions.RequestException: continue\n",
        "                except Exception: continue\n",
        "    if entity_docs: contents,metas = zip(*entity_docs); upsert_to_kb(contents,metas)\n",
        "    return yt_data, ner\n"
      ],
      "metadata": {
        "id": "PYvyH-moulYS"
      },
      "id": "PYvyH-moulYS",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UnY1Oj3Z1sIZ",
      "metadata": {
        "id": "UnY1Oj3Z1sIZ"
      },
      "outputs": [],
      "source": [
        "# @title 4. Agentic Transcript Processor\n",
        "import chroma_client\n",
        "\n",
        "def get_error_kb_collection():\n",
        "    return chroma_client.get_or_create_collection(name=\"error_kb\", metadata={\"hnsw:space\": \"cosine\"})\n",
        "\n",
        "def upsert_error_to_kb(error_text, language):\n",
        "    get_error_kb_collection().upsert(\n",
        "        ids=[_id_for(f\"{language}|{error_text}\")],\n",
        "        documents=[error_text],\n",
        "        metadatas=[{\"language\": language, \"type\": \"subtitle_error\"}],\n",
        "        embeddings=embed_texts([error_text])\n",
        "    )\n",
        "\n",
        "def retrieve_relevant_errors(query_text, language, top_k=5):\n",
        "    r = get_error_kb_collection().query(\n",
        "        query_embeddings=embed_texts([query_text]),\n",
        "        n_results=top_k,\n",
        "        where={\"language\": language},\n",
        "        include=[\"documents\"]\n",
        "    )\n",
        "    return r[\"documents\"][0] if r[\"documents\"] else []\n",
        "\n",
        "def format_as_srt_from_segments(segments: list[dict]) -> str:\n",
        "    def format_time(seconds):\n",
        "        if \":\" in str(seconds): return seconds\n",
        "        h = int(seconds // 3600); m = int((seconds % 3600) // 60)\n",
        "        s = int(seconds % 60); ms = int((seconds - int(seconds)) * 1000)\n",
        "        return f\"{h:02}:{m:02}:{s:02},{ms:03d}\"\n",
        "\n",
        "    srt_lines = []\n",
        "    for idx, seg in enumerate(segments, 1):\n",
        "        start, end, text = format_time(seg[\"start\"]), format_time(seg[\"end\"]), str(seg[\"text\"]).replace(\"\\n\", \" \").strip()\n",
        "        srt_lines.append(f\"{idx}\\n{start} --> {end}\\n{text}\\n\")\n",
        "    return \"\\n\".join(srt_lines)\n",
        "\n",
        "def ollama_chat(messages, temp=0.3):\n",
        "    return ollama.chat(model=llm_model, messages=messages, options={'temperature': temp})['message']['content']\n",
        "\n",
        "def validate_subtitles(lang, srt_text, kb_context):\n",
        "    rules = {\"tamil\": \"- Entities must match KB. - No empty lines. - Valid SRT numbering/timestamps.\",\n",
        "             \"english\": \"- Numbering/timestamps intact. - No empty lines. - Entities must match KB.\"}[lang]\n",
        "    prompt = f\"Validate {lang.capitalize()} subtitles:\\n{rules}\\nReply: PASS or FAIL: <issues>.\\n{srt_text[:4000]}\"\n",
        "    role = \"subtitle validator\" if lang==\"tamil\" else \"subtitle QA agent\"\n",
        "    return ollama_chat([{\"role\": \"system\", \"content\": f\"You are a {role}.\"},\n",
        "                        {\"role\": \"user\", \"content\": prompt}], temp=0).strip()\n",
        "\n",
        "def parse_srt(srt_file):\n",
        "    segments = []\n",
        "    with open(srt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for block in f.read().strip().split(\"\\n\\n\"):\n",
        "            lines = block.split(\"\\n\")\n",
        "            if len(lines) >= 3:\n",
        "                start, end = lines[1].split(\" --> \")\n",
        "                segments.append({\"start\": start, \"end\": end, \"text\": \" \".join(lines[2:])})\n",
        "    return segments\n",
        "\n",
        "def retrieve_kb_context(kb_collection, query_text=\"video context + named entities\", top_k=10):\n",
        "    if not kb_collection: return \"\"\n",
        "    results = kb_collection.query(query_embeddings=embed_texts([query_text]), n_results=top_k, include=[\"documents\"])\n",
        "    return \"\\n\".join(docs for docs_per_query in results.get(\"documents\", []) for docs in docs_per_query)\n",
        "\n",
        "def process_subtitles(lang, kb_context, errors, base_segments=None, merged_texts=None, prev_srt=None, max_loops=3):\n",
        "    for loop in range(max_loops):\n",
        "        print(f\"Loop {loop+1}: Processing {lang.capitalize()} subtitles...\")\n",
        "        if lang == \"tamil\":\n",
        "            prompt = f\"\"\"You are an autonomous AI subtitle agent.\n",
        "Task: Create polished Tamil subtitles from noisy WhisperX transcripts.\n",
        "Knowledge Context: - KB Entities: {kb_context} - Past Tamil issues: {errors}\n",
        "Rules: - Follow transcript_chunk6.srt timestamps & speaker labels exactly.\n",
        "- Compare 3s/6s/15s/30s transcripts. - Correct entities via KB+errors.\n",
        "- Maintain numbering/order. - Output valid Tamil SRT only.\n",
        "Transcripts:\\n{merged_texts}\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"Translate Tamil subtitles into English SRT.\n",
        "Knowledge Context: - KB Entities: {kb_context} - Past English issues: {errors}\n",
        "Rules: - Follow transcript_chunk6.srt timestamps & speaker labels exactly.\n",
        "- Preserve numbering/timestamps. - Keep colloquial tone. - Correct entities via KB+errors.\n",
        "Tamil SRT:\\n{prev_srt[:4000]}\"\"\"\n",
        "\n",
        "        result = ollama_chat([{\"role\": \"system\", \"content\": f\"You are a {lang} subtitle agent.\"},\n",
        "                              {\"role\": \"user\", \"content\": prompt}])\n",
        "\n",
        "        if lang == \"tamil\":\n",
        "            lines = [l.strip() for l in result.split(\"\\n\") if l.strip()]\n",
        "            result = format_as_srt_from_segments([{\"start\": seg[\"start\"], \"end\": seg[\"end\"], \"text\": line}\n",
        "                                                  for seg, line in zip(base_segments, lines)])\n",
        "\n",
        "        verdict = validate_subtitles(lang, result, kb_context)\n",
        "        print(f\"{lang.capitalize()} validation verdict: {verdict}\")\n",
        "        if verdict.startswith(\"PASS\"): return result\n",
        "        print(f\"Issues in {lang}, refining...\"); upsert_error_to_kb(verdict, lang)\n",
        "    return result\n",
        "\n",
        "def agentic_tamil_english_subtitles(kb_collection, transcript_srt_files, max_loops=3):\n",
        "    print(\"Agentic Tamil + English subtitle consolidation with KB-backed error memory...\")\n",
        "    kb_context = retrieve_kb_context(kb_collection)\n",
        "    base_segments = parse_srt(transcript_srt_files[-1])\n",
        "    merged_texts = [\" \".join(seg[\"text\"] for seg in parse_srt(fp)) for fp in transcript_srt_files]\n",
        "\n",
        "    tamil_srt = process_subtitles(\"tamil\", kb_context, retrieve_relevant_errors(\"Tamil transcript errors\", \"tamil\"),\n",
        "                                 base_segments=base_segments, merged_texts=merged_texts, max_loops=max_loops)\n",
        "    open(f\"{LLM_DIR}/Tamil_subtitles.srt\", \"w\", encoding=\"utf-8\").write(tamil_srt)\n",
        "\n",
        "    english_srt = process_subtitles(\"english\", kb_context, retrieve_relevant_errors(\"English subtitle issues\", \"english\"),\n",
        "                                   prev_srt=tamil_srt, base_segments=base_segments, max_loops=max_loops)\n",
        "    open(f\"{LLM_DIR}/English_subtitles.srt\", \"w\", encoding=\"utf-8\").write(english_srt)\n",
        "\n",
        "    return f\"{LLM_DIR}/Tamil_subtitles.srt\", f\"{LLM_DIR}/English_subtitles.srt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Agentic Tamil/English Subtitle Pipeline\n",
        "\n",
        "def process_youtube_video(youtube_url):\n",
        "    print(\"\\n▶ Step 1: Extract audio\")\n",
        "    audio_path = extract_youtube_audio(youtube_url, AUDIO_DIR)\n",
        "\n",
        "    print(\"\\n▶ Step 2: Transcribe audio with WhisperX\")\n",
        "    srt_file_list = list(transcribe_audio(\n",
        "        audio_file=audio_path, output_dir=WHISPERX_DIR, hf_token=userdata.get(\"HF_TOKEN\"),\n",
        "        chunk_sizes=[3, 6, 15, 30], output_format=\"srt\"\n",
        "    ).values())\n",
        "\n",
        "    print(\"\\n▶ Step 3: Scrape YouTube metadata + NER\")\n",
        "    yt_data, ner = scrape_youtube_extract_ner(youtube_url)\n",
        "\n",
        "    print(\"\\n▶ Step 4: Agentic AI subtitle processing\")\n",
        "    return agentic_tamil_english_subtitles(kb_collection=kb, transcript_srt_files=srt_file_list)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tamil_srt, english_srt = process_youtube_video(\"https://www.youtube.com/watch?v=Cz_9yG2FQ3E&ab_channel=ALWAYSREACTION\")"
      ],
      "metadata": {
        "id": "XPCVLi2nFC0S"
      },
      "id": "XPCVLi2nFC0S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. RAG Q&A function\n",
        "def rag_query(question, language=\"english\"):\n",
        "    print(f\"Answering question in {language}: {question}\")\n",
        "    results = kb.query(query_embeddings=[embed_texts([question])[0]], n_results=5, include=[\"documents\",\"metadatas\"])\n",
        "    context = \"\".join(f\"[Source {i+1}]: {doc}\\n\\n\" for i, doc in enumerate(results[\"documents\"][0]))\n",
        "\n",
        "    prompts = {\n",
        "        \"tamil\": f\"கீழ்க்கண்ட தகவல்களைப் பயன்படுத்தி கேள்விக்கு பதிலளிக்கவும். தமிழில் மட்டுமே பதிலளிக்கவும்.\\nதகவல்: {context}\\nகேள்வி: {question}\\nபதில்:\",\n",
        "        \"english\": f\"Use this information to answer the question in English only.\\nContext: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        return ollama.generate(model=llm_model, prompt=prompts.get(language.lower(),\"english\"), options={'temperature':0.2,'top_p':0.9})['response']\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {e}\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(rag_query(\"Summarize the video and list key people\", language=\"english\"))\n",
        "    #print(rag_query(\"இந்த வீடியோவின் முக்கிய நபர்களை சொல்லுங்கள்\", language=\"tamil\"))"
      ],
      "metadata": {
        "id": "EAhBU0cWE291"
      },
      "id": "EAhBU0cWE291",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}